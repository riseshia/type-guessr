#!/usr/bin/env ruby
# frozen_string_literal: true

# Profile TypeGuessr performance using stackprof
#
# Usage:
#   bin/profile [options]
#
# Options:
#   --target=indexing|inference|all  What to profile (default: all)
#   --mode=cpu|wall|object           Profiling mode (default: cpu)
#   --limit=N                        Max files to process (default: all)
#   --samples=N                      Number of inference samples (default: 100)
#   --output=path                    Output file path (default: tmp/stackprof-<target>.dump)
#   --interval=N                     Sampling interval in microseconds (default: 10)
#   --path=PATH                      Project directory to profile (default: current)
#   --report                         Generate markdown report
#
# Examples:
#   bin/profile                               # Profile everything
#   bin/profile --target=indexing             # Profile indexing only
#   bin/profile --target=inference            # Profile type inference only
#   bin/profile --mode=object                 # Profile object allocations
#   bin/profile --report                      # Generate markdown report
#   bin/profile --path=/path/to/rails         # Profile a different project

require "bundler/setup"
require "stackprof"
require "prism"
require "optparse"
require "fileutils"
require "ruby_indexer/ruby_indexer"

# Load TypeGuessr core components
require_relative "../lib/type_guessr/core/ir/nodes"
require_relative "../lib/type_guessr/core/types"
require_relative "../lib/type_guessr/core/converter/prism_converter"
require_relative "../lib/type_guessr/core/index/location_index"
require_relative "../lib/type_guessr/core/inference/resolver"
require_relative "../lib/type_guessr/core/rbs_provider"
require_relative "../lib/type_guessr/core/signature_provider"

module ProfileIndexing
  # Runs stackprof profiling on TypeGuessr
  class Runner
    TARGET_DESCRIPTIONS = {
      indexing: "indexing only",
      inference: "inference only",
      all: "indexing + inference"
    }.freeze

    def initialize(target:, mode:, limit:, samples:, output:, interval:, path:, report:)
      @target = target.to_sym
      @mode = mode.to_sym
      @limit = limit
      @samples = samples
      @output = output || default_output
      @interval = interval
      @path = path
      @report = report
      @converter = TypeGuessr::Core::Converter::PrismConverter.new
      @location_index = TypeGuessr::Core::Index::LocationIndex.new
      @resolver = build_resolver
      @indexed_nodes = []
      @results = {}
    end

    def run
      puts "Project path: #{@path || Dir.pwd}"
      files = collect_indexable_files
      puts "Found #{files.size} indexable files (same as ruby-lsp)"

      if @limit && @limit < files.size
        files = files.first(@limit)
        puts "Processing first #{@limit} files (use --limit to change)"
      end

      FileUtils.mkdir_p(File.dirname(@output))

      case @target
      when :indexing
        profile_indexing(files)
      when :inference
        # Need to index first without profiling
        puts "\nPre-indexing files (not profiled)..."
        index_files_silent(files)
        profile_inference
      when :all
        profile_all(files)
      end

      generate_report if @report
    end

    private

    def default_output
      "tmp/stackprof-#{@target}.dump"
    end

    def build_resolver
      provider = TypeGuessr::Core::SignatureProvider.new
      provider.add_provider(TypeGuessr::Core::RBSProvider.instance)
      TypeGuessr::Core::Inference::Resolver.new(provider)
    end

    def collect_indexable_files
      if @path
        # Collect files in the target project's Bundler environment
        collect_files_from_project(@path)
      else
        config = RubyIndexer::Configuration.new
        config.indexable_uris.map { |uri| uri.full_path.to_s }
      end
    end

    def collect_files_from_project(project_path)
      # Run ruby in the target project directory to get file list with its Gemfile
      script = <<~RUBY
        require 'bundler/setup'
        require 'ruby_indexer/ruby_indexer'
        config = RubyIndexer::Configuration.new
        config.indexable_uris.each { |uri| puts uri.full_path }
      RUBY

      # Clean bundler environment variables for subprocess
      clean_env = ENV.to_h.reject { |k, _| k.start_with?('BUNDLE_', 'RUBYGEMS_', 'GEM_') }
      clean_env['BUNDLE_GEMFILE'] = File.join(project_path, 'Gemfile')

      output = IO.popen(clean_env, ['ruby'], 'r+', chdir: project_path, err: [:child, :out]) do |io|
        io.write(script)
        io.close_write
        io.read
      end

      unless $?.success?
        warn "Warning: Failed to collect files from #{project_path}"
        warn "Falling back to current directory scan"
        Dir.chdir(project_path) do
          config = RubyIndexer::Configuration.new
          return config.indexable_uris.map { |uri| uri.full_path.to_s }
        end
      end

      output.lines.map(&:chomp)
    end

    # Profile indexing only
    def profile_indexing(files)
      puts "\n=== Profiling: Indexing ==="
      puts "Mode: #{@mode}, Interval: #{@interval}μs"
      puts "-" * 50

      result = StackProf.run(mode: @mode, interval: @interval, raw: true) do
        index_files(files)
      end

      @results[:indexing] = result
      print_result("Indexing", result)
      save_result(result, @output)
      print_indexing_stats
    end

    # Profile inference only (assumes already indexed)
    def profile_inference
      puts "\n=== Profiling: Type Inference ==="
      puts "Mode: #{@mode}, Interval: #{@interval}μs, Samples: #{@samples}"
      puts "-" * 50

      # Collect sample nodes for inference
      sample_nodes = collect_inference_samples

      if sample_nodes.empty?
        puts "No nodes to sample for inference!"
        return
      end

      puts "Collected #{sample_nodes.size} nodes for inference sampling"

      result = StackProf.run(mode: @mode, interval: @interval, raw: true) do
        run_inference(sample_nodes)
      end

      @results[:inference] = result
      print_result("Inference", result)
      save_result(result, @output.sub(".dump", "-inference.dump"))
      print_inference_stats(sample_nodes)
    end

    # Profile everything (indexing and inference separately, with both CPU and wall time)
    def profile_all(files)
      # Phase 1: Indexing (CPU)
      puts "\n=== Profiling: Indexing (CPU) ==="
      puts "Mode: #{@mode}, Interval: #{@interval}μs"
      puts "-" * 50

      indexing_result = StackProf.run(mode: @mode, interval: @interval, raw: true) do
        index_files(files)
      end

      @results[:indexing] = indexing_result
      print_result("Indexing (CPU)", indexing_result)
      save_result(indexing_result, @output.sub(".dump", "-indexing.dump"))
      print_indexing_stats

      # Phase 1b: Indexing (Wall) - if CPU mode, also run wall time
      if @mode == :cpu
        puts "\n=== Profiling: Indexing (Wall Time) ==="
        puts "Mode: wall, Interval: #{@interval}μs"
        puts "-" * 50

        # Reset for wall time profiling
        reset_state
        indexing_wall_result = StackProf.run(mode: :wall, interval: @interval, raw: true) do
          index_files(files)
        end

        @results[:indexing_wall] = indexing_wall_result
        print_result("Indexing (Wall)", indexing_wall_result)
        save_result(indexing_wall_result, @output.sub(".dump", "-indexing-wall.dump"))
      end

      # Phase 2: Inference (CPU)
      puts "\n=== Profiling: Inference (CPU) ==="
      puts "Mode: #{@mode}, Interval: #{@interval}μs, Samples: #{@samples}"
      puts "-" * 50

      sample_nodes = collect_inference_samples
      if sample_nodes.empty?
        puts "No nodes to sample for inference!"
        return
      end

      puts "Collected #{sample_nodes.size} nodes for inference sampling"

      inference_result = StackProf.run(mode: @mode, interval: @interval, raw: true) do
        run_inference(sample_nodes)
      end

      @results[:inference] = inference_result
      print_result("Inference (CPU)", inference_result)
      save_result(inference_result, @output.sub(".dump", "-inference.dump"))
      print_inference_stats(sample_nodes)

      # Phase 2b: Inference (Wall) - if CPU mode, also run wall time
      return unless @mode == :cpu

      puts "\n=== Profiling: Inference (Wall Time) ==="
      puts "Mode: wall, Interval: #{@interval}μs"
      puts "-" * 50

      inference_wall_result = StackProf.run(mode: :wall, interval: @interval, raw: true) do
        run_inference(sample_nodes)
      end

      @results[:inference_wall] = inference_wall_result
      print_result("Inference (Wall)", inference_wall_result)
      save_result(inference_wall_result, @output.sub(".dump", "-inference-wall.dump"))
    end

    def reset_state
      @converter = TypeGuessr::Core::Converter::PrismConverter.new
      @location_index = TypeGuessr::Core::Index::LocationIndex.new
      @indexed_nodes = []
    end

    def index_files(files)
      files.each do |file_path|
        process_file(file_path)
      rescue StandardError => e
        warn "Error processing #{file_path}: #{e.message}"
      end
      @location_index.finalize!
    end

    def index_files_silent(files)
      files.each do |file_path|
        process_file(file_path)
      rescue StandardError
        # Ignore errors during pre-indexing
      end
      @location_index.finalize!
    end

    def process_file(file_path)
      source = File.read(file_path)
      parsed = Prism.parse(source)
      return unless parsed.value

      context = TypeGuessr::Core::Converter::PrismConverter::Context.new
      nodes = parsed.value.statements&.body&.filter_map do |stmt|
        @converter.convert(stmt, context)
      end

      @location_index.remove_file(file_path)
      nodes&.each { |node| index_node_recursively(file_path, node, "") }
    end

    def index_node_recursively(file_path, node, scope_id)
      return unless node

      @location_index.add(file_path, node, scope_id)

      # Track nodes for inference sampling
      @indexed_nodes << [node, scope_id] if should_sample_node?(node)

      case node
      when TypeGuessr::Core::IR::ClassModuleNode
        new_scope = scope_id.empty? ? node.name : "#{scope_id}::#{node.name}"
        node.methods&.each do |method|
          if method.is_a?(TypeGuessr::Core::IR::ClassModuleNode)
            index_node_recursively(file_path, method, scope_id.empty? ? node.name : "#{scope_id}::#{node.name}")
          else
            index_node_recursively(file_path, method, new_scope)
          end
        end

      when TypeGuessr::Core::IR::DefNode
        new_scope = scope_id.empty? ? "##{node.name}" : "#{scope_id}##{node.name}"
        node.params&.each { |param| index_node_recursively(file_path, param, new_scope) }
        node.body_nodes&.each { |body_node| index_node_recursively(file_path, body_node, new_scope) }

      when TypeGuessr::Core::IR::LocalWriteNode
        index_node_recursively(file_path, node.value, scope_id) if node.value

      when TypeGuessr::Core::IR::InstanceVariableWriteNode
        index_node_recursively(file_path, node.value, scope_id) if node.value

      when TypeGuessr::Core::IR::ClassVariableWriteNode
        index_node_recursively(file_path, node.value, scope_id) if node.value

      when TypeGuessr::Core::IR::CallNode
        index_node_recursively(file_path, node.receiver, scope_id) if node.receiver
        node.args&.each { |arg| index_node_recursively(file_path, arg, scope_id) }
        node.block_params&.each { |param| index_node_recursively(file_path, param, scope_id) }
        index_node_recursively(file_path, node.block_body, scope_id) if node.block_body

      when TypeGuessr::Core::IR::ParamNode
        index_node_recursively(file_path, node.default_value, scope_id) if node.default_value

      when TypeGuessr::Core::IR::MergeNode
        node.branches&.each { |branch| index_node_recursively(file_path, branch, scope_id) }

      when TypeGuessr::Core::IR::ReturnNode
        index_node_recursively(file_path, node.value, scope_id) if node.value

      when TypeGuessr::Core::IR::ConstantNode
        index_node_recursively(file_path, node.dependency, scope_id) if node.dependency
      end
    end

    def should_sample_node?(node)
      # Sample nodes that are interesting for type inference
      case node
      when TypeGuessr::Core::IR::DefNode,
           TypeGuessr::Core::IR::LocalWriteNode,
           TypeGuessr::Core::IR::LocalReadNode,
           TypeGuessr::Core::IR::ParamNode,
           TypeGuessr::Core::IR::CallNode
        true
      else
        false
      end
    end

    def collect_inference_samples
      return [] if @indexed_nodes.empty?

      # Sample evenly from the indexed nodes
      step = [@indexed_nodes.size / @samples, 1].max
      @indexed_nodes.each_slice(step).map(&:first).first(@samples)
    end

    def run_inference(sample_nodes)
      sample_nodes.each do |(node, _scope_id)|
        @resolver.infer(node)
      rescue StandardError
        # Ignore inference errors during profiling
      end
    end

    def print_result(label, result)
      puts "\n#{label} Profile:"
      StackProf::Report.new(result).print_text($stdout)
    end

    def save_result(result, path)
      File.binwrite(path, Marshal.dump(result))
      puts "\n#{"-" * 50}"
      puts "Profile saved to: #{path}"
      puts "\nTo view detailed report:"
      puts "  stackprof #{path}"
      puts "  stackprof --d3-flamegraph #{path} > #{path.sub(".dump", ".html")}"
    end

    def print_indexing_stats
      stats = @location_index.stats
      puts "\nIndexing Statistics:"
      puts "  Files indexed: #{stats[:files_count]}"
      puts "  Total nodes: #{stats[:total_nodes]}"
      puts "  Nodes sampled for inference: #{@indexed_nodes.size}"
    end

    def print_inference_stats(sample_nodes)
      puts "\nInference Statistics:"
      puts "  Nodes sampled: #{sample_nodes.size}"

      # Count by node type
      by_type = sample_nodes.group_by { |node, _| node.class.name.split("::").last }
      by_type.each do |type, nodes|
        puts "    #{type}: #{nodes.size}"
      end
    end

    def generate_report
      puts "\n=== Generating Markdown Report ==="

      report_path = "tmp/profile-report.md"
      File.write(report_path, build_report_content)
      puts "Report saved to: #{report_path}"
    end

    def build_report_content
      <<~MARKDOWN
        # TypeGuessr Performance Profile Report

        Generated: #{Time.now.strftime("%Y-%m-%d %H:%M:%S")}

        ## Configuration

        - **Target:** #{@target} (#{target_description})
        - **Mode:** #{@mode} (#{@interval}μs interval)
        - **Files indexed:** #{@location_index.stats[:files_count]}
        - **Total nodes:** #{@location_index.stats[:total_nodes]}
        - **Inference samples:** #{@samples}

        ---

        #{indexing_report_section}

        #{inference_report_section}

        ## Call Graph Analysis

        ### Indexing Pipeline

        ```
        ProfileIndexing::Runner#index_files
          └── ProfileIndexing::Runner#process_file
                ├── Prism.parse
                └── PrismConverter#convert
                      ├── convert_class_or_module
                      │     ├── convert_def
                      │     ├── convert_array_literal
                      │     └── convert_call
                      └── convert_loc
                            └── Prism::Source#find_line
        ```

        ### Inference Pipeline

        ```
        Resolver#infer
          ├── infer_call
          │     └── RBSProvider#get_method_signatures
          │           └── RBS::DefinitionBuilder
          ├── infer_local_write
          └── infer_local_read
        ```

        ---

        #{optimization_recommendations_section}

        ---

        ## Generated Artifacts

        | File | Description |
        |------|-------------|
        #{dump_files_section}

        ## How to Further Analyze

        ```bash
        # View detailed text report
        stackprof tmp/stackprof-all-indexing.dump
        stackprof tmp/stackprof-all-inference.dump

        # Analyze specific method
        stackprof tmp/stackprof-all-indexing.dump --method 'PrismConverter#convert_loc'

        # Walk call tree for a method
        stackprof tmp/stackprof-all-indexing.dump --method 'Prism::Source#find_line' --walk

        # Generate interactive flamegraph
        stackprof --d3-flamegraph tmp/stackprof-all-indexing.dump > tmp/flamegraph-indexing.html
        stackprof --d3-flamegraph tmp/stackprof-all-inference.dump > tmp/flamegraph-inference.html
        ```
      MARKDOWN
    end

    def indexing_report_section
      result = @results[:indexing]
      return "## Indexing\n\nNo indexing profile data available.\n" unless result

      frames = extract_top_frames(result)
      gc_stats = extract_gc_stats(result)

      section = <<~SECTION
        ## Indexing Profile

        ### Key Metrics (CPU)

        | Metric | Value |
        |--------|-------|
        | Total samples | #{result[:samples]} |
        | GC samples | #{gc_stats[:total]} (#{gc_stats[:percent]}%) |
        | Miss rate | #{format("%.2f%%", (result[:missed_samples] || 0).to_f / [result[:samples], 1].max * 100)} |

        ### Top CPU Bottlenecks

        #{top_bottlenecks_section(frames, result, :indexing)}

      SECTION

      # Add wall time analysis if available
      wall_result = @results[:indexing_wall]
      if wall_result
        wall_frames = extract_top_frames(wall_result)
        io_analysis = analyze_io_wait(result, wall_result)

        section += <<~WALL

          ### Wall Time Analysis

          #{io_analysis}

          ### Top Wall Time Bottlenecks

          #{top_bottlenecks_section(wall_frames, wall_result, :indexing)}

        WALL
      end

      "#{section}---\n\n"
    end

    def inference_report_section
      result = @results[:inference]
      return "## Inference\n\nNo inference profile data available.\n" unless result

      frames = extract_top_frames(result)
      gc_stats = extract_gc_stats(result)

      section = <<~SECTION
        ## Inference Profile

        ### Key Metrics (CPU)

        | Metric | Value |
        |--------|-------|
        | Total samples | #{result[:samples]} |
        | GC samples | #{gc_stats[:total]} (#{gc_stats[:percent]}%) |
        | Miss rate | #{format("%.2f%%", (result[:missed_samples] || 0).to_f / [result[:samples], 1].max * 100)} |

        ### Top CPU Bottlenecks

        #{top_bottlenecks_section(frames, result, :inference)}

      SECTION

      # Add wall time analysis if available
      wall_result = @results[:inference_wall]
      if wall_result
        wall_frames = extract_top_frames(wall_result)
        io_analysis = analyze_io_wait(result, wall_result)

        section += <<~WALL

          ### Wall Time Analysis

          #{io_analysis}

          ### Top Wall Time Bottlenecks

          #{top_bottlenecks_section(wall_frames, wall_result, :inference)}

        WALL
      end

      "#{section}---\n\n"
    end

    def analyze_io_wait(cpu_result, wall_result)
      cpu_result[:samples] || 1
      wall_samples = wall_result[:samples] || 1

      # Find I/O related frames in wall time
      wall_frames = extract_top_frames(wall_result)
      io_frames = wall_frames.select { |f| f[:name] =~ /IO\.|File\.|read|write|open/i }
      io_percent = io_frames.sum { |f| f[:percent] }

      # Compare CPU vs Wall for key methods
      cpu_frames = extract_top_frames(cpu_result)
      comparisons = []

      %w[Prism.parse IO.read RBS::Parser].each do |method_pattern|
        cpu_frame = cpu_frames.find { |f| f[:name]&.include?(method_pattern) }
        wall_frame = wall_frames.find { |f| f[:name]&.include?(method_pattern) }
        next unless cpu_frame && wall_frame

        cpu_pct = cpu_frame[:percent]
        wall_pct = wall_frame[:percent]
        diff = wall_pct - cpu_pct

        if diff.abs > 1.0
          comparisons << "| `#{method_pattern}` | #{format("%.1f", cpu_pct)}% | #{format("%.1f", wall_pct)}% " \
                         "| #{format("%+.1f", diff)}% |"
        end
      end

      result = "| Metric | Value |\n|--------|-------|\n"
      result += "| Wall samples | #{wall_samples} |\n"
      result += "| I/O related | #{format("%.1f", io_percent)}% |\n"

      if comparisons.any?
        result += "\n**CPU vs Wall Comparison** (methods with >1% difference):\n\n"
        result += "| Method | CPU % | Wall % | Diff |\n"
        result += "|--------|-------|--------|------|\n"
        result += comparisons.join("\n")
      end

      result
    end

    def optimization_recommendations_section
      indexing_frames = @results[:indexing] ? extract_top_frames(@results[:indexing]) : []
      inference_frames = @results[:inference] ? extract_top_frames(@results[:inference]) : []

      <<~SECTION
        ## Optimization Recommendations

        #{optimization_recommendations(indexing_frames, inference_frames)}
      SECTION
    end

    def target_description
      TARGET_DESCRIPTIONS[@target]
    end

    def extract_top_frames(result)
      return [] unless result && result[:frames]

      frames = result[:frames].map do |_frame_id, data|
        {
          name: data[:name],
          samples: data[:samples] || 0,
          total_samples: data[:total_samples] || 0,
          percent: (data[:samples] || 0).to_f / [result[:samples], 1].max * 100,
          total_percent: (data[:total_samples] || 0).to_f / [result[:samples], 1].max * 100
        }
      end
      frames.sort_by { |f| -f[:samples] }
    end

    def extract_gc_stats(result)
      return { total: 0, percent: 0.0, marking: 0, sweeping: 0 } unless result

      gc_samples = result[:gc_samples] || 0
      total = result[:samples] || 1

      # Try to find marking/sweeping from frames
      frames = result[:frames] || {}
      marking = frames.values.find { |f| f[:name] == "(marking)" }&.dig(:samples) || 0
      sweeping = frames.values.find { |f| f[:name] == "(sweeping)" }&.dig(:samples) || 0

      {
        total: gc_samples,
        percent: format("%.2f", gc_samples.to_f / total * 100),
        marking: marking,
        sweeping: sweeping
      }
    end

    def executive_summary(result, gc_stats)
      return "No profiling data available." unless result

      find_line_pct = find_line_percent(extract_top_frames(result))

      <<~SUMMARY
        TypeGuessr performance profiling results show that **#{find_line_pct}% of CPU time is spent in `Prism::Source#find_line`**, which represents the biggest optimization opportunity. GC overhead is #{gc_stats[:percent]}%, indicating significant memory pressure from object allocations.

        **Key findings:**
        - `Prism::Source#find_line` is the #1 bottleneck (location conversion for every node)
        - `IR::Loc.new` creates millions of location objects, contributing to GC pressure
        - Inference accounts for only ~3% of total time; indexing dominates
      SUMMARY
    end

    def find_line_percent(frames)
      frame = frames.find { |f| f[:name]&.include?("find_line") }
      frame ? format("%.1f", frame[:percent]) : "N/A"
    end

    def bottleneck_detail(frame, rank, _result, phase = nil)
      name = frame[:name]
      pct = format("%.1f", frame[:percent])
      samples = frame[:samples]
      info = bottleneck_info(name, phase)

      "| #{rank} | `#{name}` | #{pct}% | #{samples} | #{info[:where]} | #{info[:what]} |"
    end

    def bottleneck_info(name, phase)
      case name
      when /Prism\.parse/
        files = @location_index.stats[:files_count]
        avg = @location_index.stats[:total_nodes] / [files, 1].max
        { where: "process_file", what: "소스→AST 변환 (#{files}파일, ~#{avg}노드/파일). 병렬화 가능" }
      when /Prism::Source#find_line/
        { where: "convert_loc", what: "바이트 오프셋→라인 번호. 노드마다 호출" }
      when /bsearch_index/
        { where: "find_line 내부", what: "라인 번호 이진 탐색. 누적 비용 높음" }
      when /PrismConverter#convert_loc/
        { where: "모든 convert_*", what: "IR::Loc 생성. find_line+column 트리거" }
      when /PrismConverter#convert_class/
        { where: "convert 진입점", what: "클래스/모듈 변환. 내부 메서드 재귀 처리" }
      when /PrismConverter#convert_def/
        { where: "convert_class_or_module", what: "메서드 정의 변환. 파라미터+본문+반환값" }
      when /PrismConverter#convert_call/
        { where: "convert_*", what: "메서드 호출 변환. duck typing용 추적" }
      when /PrismConverter#convert$/
        { where: "process_file", what: "Prism AST→IR 노드 그래프 진입점" }
      when /LocationIndex#add/
        { where: "index_node_recursively", what: "노드를 인덱스에 추가. hash+uniq 포함" }
      when /Class#new|Data#initialize/
        { where: "IR 노드 생성", what: "객체 할당 (Loc, LiteralNode, CallNode 등)" }
      when /Array#uniq/
        { where: "LocationIndex#add", what: "중복 노드 제거. Set으로 대체 권장" }
      when /Data#hash|Array#hash/
        { where: "LocationIndex", what: "노드 키 해시 계산 (조회/저장용)" }
      when /Prism::Node#location/
        { where: "convert_loc", what: "Prism 노드에서 위치 정보 추출" }
      when /IO\.read/
        { where: "process_file", what: "파일 I/O. Wall time에서 비중 높음" }
      when /RBS::Parser/
        { where: "ensure_environment_loaded", what: "RBS 파일 파싱. 첫 추론 시 지연 로드" }
      when /RBS::DefinitionBuilder#build/
        { where: "get_method_signatures", what: "클래스 메서드 정의 빌드. 캐시됨" }
      when /RBS::DefinitionBuilder/
        { where: "RBSProvider", what: "RBS 타입 정의 구축. 첫 조회 후 캐시" }
      when /RBS::Environment/
        { where: "DefinitionBuilder", what: "RBS 환경 (타입 해석, 선언 등록)" }
      when /RBSProvider|SignatureProvider/
        { where: "Resolver#infer_call", what: "RBS에서 메서드 시그니처 조회" }
      when /Resolver#infer_call/
        { where: "infer_node", what: "메서드 호출 타입 추론. RBS 조회 포함" }
      when /Resolver#infer_local/
        { where: "infer_node", what: "지역 변수 읽기/쓰기 타입 추론" }
      when /Resolver#infer/
        { where: "Hover#type_at", what: "노드→타입 추론. 의존성 그래프 순회" }
      when /\(marking\)/
        { where: "Ruby GC", what: "마킹 단계. 살아있는 객체 탐색" }
      when /\(sweeping\)/
        { where: "Ruby GC", what: "스윕 단계. 죽은 객체 해제" }
      when /garbage collection/
        { where: "Ruby GC", what: "가비지 컬렉션. 객체 할당 줄이면 개선" }
      else
        if phase == :inference
          { where: "추론 파이프라인", what: "RBS/타입 추론 관련 작업" }
        else
          { where: "인덱싱 파이프라인", what: "AST 변환/노드 생성 작업" }
        end
      end
    end
    # rubocop:enable Metrics/MethodLength, Metrics/CyclomaticComplexity

    def top_bottlenecks_section(frames, result, phase = nil)
      return "No frame data available." if frames.empty?

      # Get top 5 bottlenecks (excluding GC-related)
      top_frames = frames.reject { |f| f[:name]&.start_with?("(") }.first(5)

      header = <<~TABLE
        | Rank | Method | CPU % | Samples | Where | What |
        |------|--------|-------|---------|-------|------|
      TABLE

      rows = top_frames.each_with_index.map do |frame, idx|
        bottleneck_detail(frame, idx + 1, result, phase)
      end

      header + rows.join("\n")
    end

    def memory_analysis_section(frames)
      if @mode != :object
        return <<~TEXT
          Memory profiling was not performed in this run.

          To analyze memory allocations, run:
          ```bash
          bin/profile --target=indexing --mode=object --interval=1
          ```
        TEXT
      end

      # For object mode, show allocation analysis
      top_allocators = frames.first(8)

      table = top_allocators.each_with_index.map do |frame, idx|
        "| #{idx + 1} | `#{frame[:name]}` | #{frame[:samples]} | #{format("%.1f", frame[:percent])}% |"
      end.join("\n")

      <<~TEXT
        ### Top Allocators

        | Rank | Method | Allocations | % Total |
        |------|--------|-------------|---------|
        #{table}

        ### Key Allocation Hotspots

        #### `IR::Loc.new`
        Location objects are created for every node. Consider:
        - Lazy initialization (compute only when accessed)
        - Array-based representation instead of Data class

        #### `String#split` in node keys
        Node key generation involves string operations. Consider:
        - Tuple-based keys `[class, method, line, col]`
        - Integer-based ID system

        #### `IR::LiteralNode.new`
        Unavoidable for Ruby code with many literals, but consider:
        - Singleton patterns for `nil`, `true`, `false`
        - Sharing identical literal nodes
      TEXT
    end

    def optimization_recommendations(_indexing_frames, _inference_frames)
      <<~TEXT
        ### Indexing Optimizations

        #### High Impact

        1. **Parallel file processing** - `Prism.parse` and file I/O can run in parallel
           - Use `Parallel` gem or `Ractor` for concurrent parsing
           - Expected: 2-4x speedup on multi-core systems

        2. **Cache line offset table** - Reduce `find_line` overhead
           - Line offsets don't change within a file
           - Cache once per file, reuse for all nodes

        #### Medium Impact

        3. **Lightweight `IR::Loc`** - Reduce object allocation
           - Lazy evaluation: only compute line/column when accessed
           - Or use frozen arrays instead of Data class

        4. **Replace `Array#uniq` with `Set`** in LocationIndex
           - `Set#add` is O(1) vs `Array#uniq` is O(n)

        ### Inference Optimizations

        1. **RBS lazy loading** - Already implemented, but first inference is slow
           - Consider background preloading after indexing

        2. **Cache method signatures** - `DefinitionBuilder` results per class
           - Already cached by RBS, but verify cache hits

        ### Memory Optimizations

        1. **Reduce GC pressure** - Target: < 10% GC overhead
           - Pool common node types
           - Use symbols instead of strings for keys
      TEXT
    end

    def dump_files_section
      dumps = Dir.glob("tmp/stackprof-*.dump")
      return "| No profile dumps found. | |" if dumps.empty?

      dumps.map do |f|
        desc = case f
               when /all/ then "CPU profile (all phases)"
               when /indexing/ then "CPU profile (indexing)"
               when /inference/ then "CPU profile (inference)"
               when /memory/ then "Object allocation profile"
               else "Profile dump"
               end
        "| `#{f}` | #{desc} |"
      end.join("\n")
    end
  end
end

# Parse command line options
options = {
  target: "all",
  mode: "cpu",
  limit: nil,
  samples: 100,
  output: nil,
  interval: 10,
  path: nil,
  report: false
}

OptionParser.new do |opts|
  opts.banner = "Usage: #{$PROGRAM_NAME} [options]"

  opts.on("--target=TARGET", "What to profile: indexing, inference, all (default: all)") do |v|
    options[:target] = v
  end

  opts.on("--mode=MODE", "Profiling mode: cpu, wall, object (default: cpu)") do |v|
    options[:mode] = v
  end

  opts.on("--limit=N", Integer, "Max files to process") do |v|
    options[:limit] = v
  end

  opts.on("--samples=N", Integer, "Number of inference samples (default: 100)") do |v|
    options[:samples] = v
  end

  opts.on("--output=PATH", "Output file path") do |v|
    options[:output] = v
  end

  opts.on("--interval=N", Integer, "Sampling interval in microseconds (default: 10)") do |v|
    options[:interval] = v
  end

  opts.on("--path=PATH", "Project directory to profile") do |v|
    options[:path] = File.expand_path(v)
  end

  opts.on("--report", "Generate markdown report") do
    options[:report] = true
  end

  opts.on("-h", "--help", "Show this help") do
    puts opts
    exit
  end
end.parse!

runner = ProfileIndexing::Runner.new(
  target: options[:target],
  mode: options[:mode],
  limit: options[:limit],
  samples: options[:samples],
  output: options[:output],
  interval: options[:interval],
  path: options[:path],
  report: options[:report]
)

runner.run
