#!/usr/bin/env ruby
# frozen_string_literal: true

# Benchmark TypeGuessr performance (indexing & inference)
#
# Usage:
#   bin/benchmark [options]
#
# Options:
#   --target=indexing|inference|all  What to benchmark (default: all)
#   --warmup=N                       Warmup time in seconds (default: 2)
#   --time=N                         Benchmark time in seconds (default: 5)
#   --samples=N                      Number of inference samples (default: 2000)
#   --path=PATH                      Project directory to benchmark (default: current)
#   --report                         Generate markdown report
#
# Examples:
#   bin/benchmark                         # Benchmark everything
#   bin/benchmark --target=indexing       # Benchmark indexing only
#   bin/benchmark --target=inference      # Benchmark inference only
#   bin/benchmark --report                # Generate markdown report
#   bin/benchmark --path=/path/to/rails   # Benchmark a different project

require "English"
require "bundler/setup"
require "benchmark/ips"
require "prism"
require "optparse"
require "fileutils"
require "ruby_indexer/ruby_indexer"

# Load TypeGuessr core components
require_relative "../lib/type_guessr/core/ir/nodes"
require_relative "../lib/type_guessr/core/types"
require_relative "../lib/type_guessr/core/converter/prism_converter"
require_relative "../lib/type_guessr/core/index/location_index"
require_relative "../lib/type_guessr/core/inference/resolver"
require_relative "../lib/type_guessr/core/rbs_provider"
require_relative "../lib/type_guessr/core/signature_provider"

module BenchmarkRunner
  # Memory measurement helper
  module Memory
    def self.usage_kb
      `ps -o rss= -p #{Process.pid}`.to_i
    end

    def self.measure
      GC.start
      before = usage_kb
      yield
      GC.start
      after = usage_kb
      after - before
    end
  end

  # Runs benchmarks on TypeGuessr
  class Runner
    def initialize(target:, warmup:, time:, samples:, path:, report:)
      @target = target.to_sym
      @warmup = warmup
      @time = time
      @samples = samples
      @path = path
      @report = report
      @results = {}
    end

    def run
      puts "Project path: #{@path || Dir.pwd}"
      files = collect_indexable_files
      puts "Found #{files.size} indexable files\n\n"

      case @target
      when :indexing
        benchmark_indexing(files)
      when :inference
        benchmark_inference(files)
      when :all
        benchmark_indexing(files)
        puts
        benchmark_inference(files)
      end

      generate_report(files) if @report
    end

    private

    def collect_indexable_files
      if @path
        # Collect files in the target project's Bundler environment
        collect_files_from_project(@path)
      else
        config = RubyIndexer::Configuration.new
        config.indexable_uris.map { |uri| uri.full_path.to_s }
      end
    end

    def collect_files_from_project(project_path)
      # Run ruby in the target project directory to get file list with its Gemfile
      script = <<~RUBY
        require 'bundler/setup'
        require 'ruby_indexer/ruby_indexer'
        config = RubyIndexer::Configuration.new
        config.indexable_uris.each { |uri| puts uri.full_path }
      RUBY

      # Clean bundler environment variables for subprocess
      clean_env = ENV.to_h.reject { |k, _| k.start_with?("BUNDLE_", "RUBYGEMS_", "GEM_") }
      clean_env["BUNDLE_GEMFILE"] = File.join(project_path, "Gemfile")

      output = IO.popen(clean_env, ["ruby"], "r+", chdir: project_path, err: %i[child out]) do |io|
        io.write(script)
        io.close_write
        io.read
      end

      unless $CHILD_STATUS.success?
        warn "Warning: Failed to collect files from #{project_path}"
        warn "Falling back to current directory scan"
        Dir.chdir(project_path) do
          config = RubyIndexer::Configuration.new
          return config.indexable_uris.map { |uri| uri.full_path.to_s }
        end
      end

      output.lines.map(&:chomp)
    end

    def benchmark_indexing(files)
      puts "=" * 60
      puts "INDEXING BENCHMARK"
      puts "=" * 60

      # Full indexing benchmark (multiple iterations for reliability)
      puts "\n### Full Indexing (3 iterations) ###\n\n"

      times = []
      nodes_counts = []
      memory_deltas = []

      3.times do |i|
        converter = TypeGuessr::Core::Converter::PrismConverter.new
        location_index = TypeGuessr::Core::Index::LocationIndex.new

        GC.start
        memory_before = Memory.usage_kb
        start_time = Process.clock_gettime(Process::CLOCK_MONOTONIC)

        files.each do |file_path|
          source = File.read(file_path)
          parsed = Prism.parse(source)
          next unless parsed.value

          location_index.remove_file(file_path)

          # Create context with location_index injection - nodes are registered during conversion
          context = TypeGuessr::Core::Converter::PrismConverter::Context.new(
            file_path: file_path,
            location_index: location_index
          )

          parsed.value.statements&.body&.each do |stmt|
            converter.convert(stmt, context)
          end
        rescue StandardError => e
          warn "Error: #{file_path}: #{e.message}"
        end

        location_index.finalize!
        end_time = Process.clock_gettime(Process::CLOCK_MONOTONIC)

        GC.start
        memory_after = Memory.usage_kb

        elapsed = end_time - start_time
        times << elapsed
        nodes_counts << location_index.stats[:total_nodes]
        memory_deltas << (memory_after - memory_before)

        puts format("  Run %<run>d: %<time>.3f sec (%<nodes>d nodes, %<mem>+d KB)",
                    run: i + 1, time: elapsed, nodes: location_index.stats[:total_nodes], mem: memory_after - memory_before)
      end

      avg_time = times.sum / times.size
      min_time = times.min
      max_time = times.max
      total_nodes = nodes_counts.first
      avg_memory = memory_deltas.sum / memory_deltas.size

      nodes_per_sec = total_nodes / avg_time
      files_per_sec = files.size / avg_time

      @results[:indexing] = {
        files: files.size,
        nodes: total_nodes,
        time: avg_time,
        min_time: min_time,
        max_time: max_time,
        nodes_per_sec: nodes_per_sec,
        files_per_sec: files_per_sec,
        memory_kb: avg_memory,
        iterations: 3
      }

      puts "\n### Summary ###\n\n"
      puts format("Files indexed:     %d", files.size)
      puts format("Total nodes:       %d", total_nodes)
      puts format("Avg time:          %<avg>.3f sec (min: %<min>.3f, max: %<max>.3f)", avg: avg_time, min: min_time, max: max_time)
      puts format("Throughput:        %.0f nodes/sec", nodes_per_sec)
      puts format("                   %.1f files/sec", files_per_sec)
      puts format("Avg memory delta:  %+d KB", avg_memory)

      # IPS benchmark (per-file for component breakdown)
      puts "\n### Component Breakdown (IPS) ###\n\n"

      sample_files = files.sample([100, files.size].min)

      Benchmark.ips do |x|
        x.config(warmup: @warmup, time: @time)

        x.report("Prism.parse") do |n|
          file = sample_files.sample

          n.times do
            source = File.read(file)
            Prism.parse(source)
          end
        end

        x.report("PrismConverter#convert") do |n|
          converter = TypeGuessr::Core::Converter::PrismConverter.new
          file = sample_files.sample
          source = File.read(file)
          parsed = Prism.parse(source)

          n.times do
            next unless parsed.value

            context = TypeGuessr::Core::Converter::PrismConverter::Context.new
            parsed.value.statements&.body&.filter_map do |stmt|
              converter.convert(stmt, context)
            end
          end
        end

        x.compare!
      end
    end

    def benchmark_inference(files)
      puts "=" * 60
      puts "INFERENCE BENCHMARK"
      puts "=" * 60

      # First, index all files
      puts "\nPre-indexing files for inference benchmark..."

      converter = TypeGuessr::Core::Converter::PrismConverter.new
      location_index = TypeGuessr::Core::Index::LocationIndex.new
      indexed_nodes = []

      files.each do |file_path|
        source = File.read(file_path)
        parsed = Prism.parse(source)
        next unless parsed.value

        location_index.remove_file(file_path)

        # Create context with location_index injection - nodes are registered during conversion
        context = TypeGuessr::Core::Converter::PrismConverter::Context.new(
          file_path: file_path,
          location_index: location_index
        )

        parsed.value.statements&.body&.each do |stmt|
          converter.convert(stmt, context)
        end
      rescue StandardError
        # Ignore errors during pre-indexing
      end

      location_index.finalize!

      # Collect sample nodes from location_index
      location_index.each_node do |node, _scope_id|
        indexed_nodes << [node, nil] if should_sample_node?(node)
      end

      # Sample nodes for inference
      step = [indexed_nodes.size / @samples, 1].max
      sample_entries = indexed_nodes.each_slice(step).map(&:first).first(@samples)

      puts "Indexed #{files.size} files, #{indexed_nodes.size} inferrable nodes"
      puts "Sampling #{sample_entries.size} nodes for inference benchmark"

      # Build resolver
      provider = TypeGuessr::Core::SignatureProvider.new
      provider.add_provider(TypeGuessr::Core::RBSProvider.instance)

      # File open measurement (first inference per file simulates "file open" scenario)
      puts "\n### File Open Metrics (first inference per file) ###\n\n"

      # Group sample entries by file path
      entries_by_file = sample_entries.group_by { |_node, file_path| file_path }

      file_open_latencies = []
      warm_latencies = []

      # Create single resolver for realistic simulation
      resolver = TypeGuessr::Core::Inference::Resolver.new(provider)

      entries_by_file.each do |_file_path, entries|
        entries.each_with_index do |(node, _path), idx|
          start_time = Process.clock_gettime(Process::CLOCK_MONOTONIC)
          resolver.infer(node)
          end_time = Process.clock_gettime(Process::CLOCK_MONOTONIC)

          latency_ms = (end_time - start_time) * 1000

          if idx.zero?
            # First node in file -> file open latency
            file_open_latencies << latency_ms
          else
            # Subsequent nodes -> warm latency
            warm_latencies << latency_ms
          end
        rescue StandardError
          # Ignore inference errors
        end
      end

      file_open_latencies.sort!
      file_open_stats = calculate_latency_stats(file_open_latencies)

      puts format("Samples:           %d (files)", file_open_latencies.size)
      puts format("Min:               %.3f ms", file_open_stats[:min])
      puts format("Max:               %.3f ms", file_open_stats[:max])
      puts format("Avg:               %.3f ms", file_open_stats[:avg])
      puts format("Median (p50):      %.3f ms", file_open_stats[:p50])
      puts format("p95:               %.3f ms", file_open_stats[:p95])
      puts format("p99:               %.3f ms", file_open_stats[:p99])

      # Warm measurement (subsequent inferences within files)
      puts "\n### Warm Metrics (subsequent inferences) ###\n\n"

      warm_latencies.sort!
      warm_stats = calculate_latency_stats(warm_latencies)

      total_time = (file_open_latencies.sum + warm_latencies.sum) / 1000.0 # convert back to seconds
      total_inferences = file_open_latencies.size + warm_latencies.size
      inferences_per_sec = total_inferences / total_time

      @results[:inference] = {
        nodes_sampled: sample_entries.size,
        files_measured: file_open_latencies.size,
        inferred: total_inferences,
        time: total_time,
        inferences_per_sec: inferences_per_sec,
        file_open: file_open_stats,
        warm: warm_stats
      }

      puts format("Nodes inferred:    %d", warm_latencies.size)
      puts format("Min:               %.3f ms", warm_stats[:min])
      puts format("Max:               %.3f ms", warm_stats[:max])
      puts format("Avg:               %.3f ms", warm_stats[:avg])
      puts format("Median (p50):      %.3f ms", warm_stats[:p50])
      puts format("p95:               %.3f ms", warm_stats[:p95])
      puts format("p99:               %.3f ms", warm_stats[:p99])

      puts "\n### Overall Throughput ###\n\n"
      puts format("Total inferences:  %d", total_inferences)
      puts format("Total time:        %.3f seconds", total_time)
      puts format("Throughput:        %.0f inferences/sec", inferences_per_sec)

      # IPS benchmark
      puts "\n### IPS Benchmark (single node inference) ###\n\n"

      # Extract nodes from sample_entries for type-specific benchmarks
      sample_nodes = sample_entries.map(&:first)
      nodes_by_type = sample_nodes.group_by { |n| n.class.name.split("::").last }

      Benchmark.ips do |x|
        x.config(warmup: @warmup, time: @time)

        x.report("infer (mixed)") do |times|
          times.times do
            node = sample_nodes.sample
            resolver.infer(node)
          rescue StandardError
            # Ignore
          end
        end

        # Type-specific benchmarks
        %w[CallNode LocalWriteNode DefNode ParamNode].each do |type|
          type_nodes = nodes_by_type[type]
          next unless type_nodes && type_nodes.size >= 10

          x.report("infer #{type}") do |times|
            times.times do
              node = type_nodes.sample
              resolver.infer(node)
            rescue StandardError
              # Ignore
            end
          end
        end

        x.compare!
      end
    end

    def should_sample_node?(node)
      case node
      when TypeGuessr::Core::IR::DefNode,
           TypeGuessr::Core::IR::LocalWriteNode,
           TypeGuessr::Core::IR::LocalReadNode,
           TypeGuessr::Core::IR::ParamNode,
           TypeGuessr::Core::IR::CallNode
        true
      else
        false
      end
    end

    def calculate_latency_stats(sorted_latencies)
      return { min: 0, max: 0, avg: 0, p50: 0, p95: 0, p99: 0 } if sorted_latencies.empty?

      size = sorted_latencies.size
      {
        min: sorted_latencies.first,
        max: sorted_latencies.last,
        avg: sorted_latencies.sum / size,
        p50: sorted_latencies[(size * 0.50).floor],
        p95: sorted_latencies[(size * 0.95).floor],
        p99: sorted_latencies[(size * 0.99).floor]
      }
    end

    def generate_report(files)
      puts "\n=== Generating Benchmark Report ==="

      FileUtils.mkdir_p("docs")
      report_path = "docs/benchmark-report.md"
      File.write(report_path, build_report_content(files))
      puts "Report saved to: #{report_path}"
    end

    def build_report_content(files)
      <<~MARKDOWN
        # TypeGuessr Benchmark Report

        Generated: #{Time.now.strftime("%Y-%m-%d %H:%M:%S")}

        ## Configuration

        - **Target:** #{@target}
        - **Files:** #{files.size}
        - **Warmup:** #{@warmup}s
        - **Benchmark time:** #{@time}s
        - **Inference samples:** #{@samples}

        ---

        #{indexing_report_section}

        #{inference_report_section}

        ## Performance Summary

        #{performance_summary}

        ---

        ## How to Use

        ```bash
        # Full benchmark
        bin/benchmark

        # With report (saved to docs/benchmark-report.md)
        bin/benchmark --report

        # Indexing only
        bin/benchmark --target=indexing

        # Inference only
        bin/benchmark --target=inference
        ```
      MARKDOWN
    end

    def indexing_report_section
      return "" unless @results[:indexing]

      r = @results[:indexing]
      <<~SECTION
        ## Indexing Results

        | Metric | Value |
        |--------|-------|
        | Files indexed | #{r[:files]} |
        | Total nodes | #{r[:nodes]} |
        | Iterations | #{r[:iterations]} |
        | Avg time | #{format("%.3f", r[:time])} sec |
        | Min/Max time | #{format("%.3f", r[:min_time])} / #{format("%.3f", r[:max_time])} sec |
        | Throughput (nodes) | #{format("%.0f", r[:nodes_per_sec])} nodes/sec |
        | Throughput (files) | #{format("%.1f", r[:files_per_sec])} files/sec |
        | Avg memory delta | #{format("%+d", r[:memory_kb])} KB |
        | Avg time per file | #{format("%.2f", r[:time] / r[:files] * 1000)} ms |
        | Avg nodes per file | #{r[:nodes] / r[:files]} |
      SECTION
    end

    def inference_report_section
      return "" unless @results[:inference]

      r = @results[:inference]
      file_open = r[:file_open] || {}
      warm = r[:warm] || {}

      <<~SECTION
        ## Inference Results

        ### File Open (first inference per file)

        | Metric | Value |
        |--------|-------|
        | Files measured | #{r[:files_measured] || 0} |
        | Min | #{format("%.3f", file_open[:min] || 0)} ms |
        | Max | #{format("%.3f", file_open[:max] || 0)} ms |
        | Avg | #{format("%.3f", file_open[:avg] || 0)} ms |
        | Median (p50) | #{format("%.3f", file_open[:p50] || 0)} ms |
        | p95 | #{format("%.3f", file_open[:p95] || 0)} ms |
        | p99 | #{format("%.3f", file_open[:p99] || 0)} ms |

        ### Warm (subsequent inferences)

        | Metric | Value |
        |--------|-------|
        | Nodes sampled | #{r[:nodes_sampled]} |
        | Total inferred | #{r[:inferred]} |
        | Total time | #{format("%.3f", r[:time])} sec |
        | Throughput | #{format("%.0f", r[:inferences_per_sec])} inferences/sec |
        | Min | #{format("%.3f", warm[:min] || 0)} ms |
        | Max | #{format("%.3f", warm[:max] || 0)} ms |
        | Avg | #{format("%.3f", warm[:avg] || 0)} ms |
        | Median (p50) | #{format("%.3f", warm[:p50] || 0)} ms |
        | p95 | #{format("%.3f", warm[:p95] || 0)} ms |
        | p99 | #{format("%.3f", warm[:p99] || 0)} ms |
      SECTION
    end

    def performance_summary
      summary = []

      if @results[:indexing]
        r = @results[:indexing]
        summary << "- **Indexing**: #{format("%.0f", r[:nodes_per_sec])} nodes/sec " \
                   "(#{format("%.1f", r[:files_per_sec])} files/sec)"
      end

      if @results[:inference]
        r = @results[:inference]
        summary << "- **Inference**: #{format("%.0f", r[:inferences_per_sec])} inferences/sec " \
                   "(#{format("%.3f", r[:time] / r[:inferred] * 1000)} ms/inference)"
      end

      summary.join("\n")
    end
  end
end

# Parse command line options
options = {
  target: "all",
  warmup: 2,
  time: 5,
  samples: 2000,
  path: nil,
  report: false
}

OptionParser.new do |opts|
  opts.banner = "Usage: #{$PROGRAM_NAME} [options]"

  opts.on("--target=TARGET", "What to benchmark: indexing, inference, all (default: all)") do |v|
    options[:target] = v
  end

  opts.on("--warmup=N", Integer, "Warmup time in seconds (default: 2)") do |v|
    options[:warmup] = v
  end

  opts.on("--time=N", Integer, "Benchmark time in seconds (default: 5)") do |v|
    options[:time] = v
  end

  opts.on("--samples=N", Integer, "Number of inference samples (default: 2000)") do |v|
    options[:samples] = v
  end

  opts.on("--path=PATH", "Project directory to benchmark") do |v|
    options[:path] = File.expand_path(v)
  end

  opts.on("--report", "Generate markdown report") do
    options[:report] = true
  end

  opts.on("-h", "--help", "Show this help") do
    puts opts
    exit
  end
end.parse!

runner = BenchmarkRunner::Runner.new(
  target: options[:target],
  warmup: options[:warmup],
  time: options[:time],
  samples: options[:samples],
  path: options[:path],
  report: options[:report]
)

runner.run
